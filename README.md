# Qwen-Tokenizer-Pruner
ç”±äºQwenæ¨¡å‹çš„è¶…å¤§vocab_size(151936)ï¼Œåœ¨æ¨¡å‹éƒ¨ç½²å’Œå°å‹åŒ–æ—¶ä»–çš„Embeddingå’ŒLM_Headçš„æƒé‡ç»´åº¦è¿‡äºå·¨å¤§ï¼Œå› æ­¤æœ¬é¡¹ç›®æä¾›äº†ä¸€å¥—Qwenå’ŒQwen-VLçš„Tokenizerè¯è¡¨å‰ªè£æ–¹æ¡ˆã€‚

**å¦‚æœæˆ‘çš„å¼€æºé¡¹ç›®ç»™ä½ å¸¦æ¥äº†å¯å‘ï¼Œç»™äºˆæˆ‘ä¸€äº›èµåŠ©å°†å¯¹æˆ‘åç»­çš„å¼€æºå·¥ä½œæœ‰å¾ˆå¤§å¸®åŠ©ã€‚**

[æ”¯æŒæˆ‘çš„åç»­å¼€æºå·¥ä½œâ¤ï¸ğŸ™](https://kaihuatang.github.io/donate.html)

# ä»£ç æ›´æ–°ä¸­

```
python main.py --old_model_path XXX --new_model_path XXX --support_data ./data --support_lang 'zh-cn' 'en'
```